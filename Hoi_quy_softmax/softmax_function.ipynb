{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    #z: (N,C)\n",
    "    e_z = np.exp(z)\n",
    "    A = e_z/e_z.sum(axis=1, keepdims = True)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(z):\n",
    "    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    A = e_z /e_z.sum(axis=1, keepdims=True)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X,y,w):\n",
    "    '''\n",
    "    X: (N,d)\n",
    "    y: (N,)\n",
    "    w: (d,C)\n",
    "        A = softmax_stable(np.dot(X,w))\n",
    "        m,n = A.shape\n",
    "        J=0.\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if j==y[i]:\n",
    "                    J+=np.log(A[i,j])\n",
    "        \n",
    "        J = -J/m\n",
    "        return J \n",
    "    '''\n",
    "    A = softmax_stable(X.dot(w))\n",
    "    id0 = range(X.shape[0]) \n",
    "    return-np.mean(np.log(A[id0, y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_gradient(X,y,w):\n",
    "    '''\n",
    "    w: (d,C)\n",
    "    X: (N,d)\n",
    "    y: (N,)\n",
    "    '''\n",
    "    A = softmax_stable(X.dot(w)) # (N,C)\n",
    "    id0 = range(A.shape[0])\n",
    "    A[id0,y] -=1 # mỗi đầu ra đúng thì trừ đi 1: err\n",
    "    return X.T.dot(A)/X.shape[0]  # 1/N *X* E.T : E chính là A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_fit(X,y,w_in,lr=0.01, nepochs=100, tol=1e-5, batch_size=10):\n",
    "    w=w_old = w_in.copy()\n",
    "    it=0\n",
    "    J_hist = [compute_cost(X,y,w)]\n",
    "    N=X.shape[0]\n",
    "    nbatches = int(np.ceil(float(N)/batch_size)) \n",
    "    #tính số lượng batch cần thiết để lặp qua toàn bộ dữ liệu\n",
    "    \n",
    "    while it < nepochs:\n",
    "        it+=1\n",
    "        mix_ids = np.random.permutation(N)\n",
    "        for i in range(nbatches):\n",
    "            # lấy chỉ số dữ liệu trong batch hiện tại\n",
    "            batch_ids = mix_ids[batch_size*i : min(batch_size*(i+1), N)]\n",
    "            X_batch, y_batch = X[batch_ids], y[batch_ids]\n",
    "            w-= lr * softmax_gradient(X_batch,y_batch,w)\n",
    "            J_hist.append(compute_cost(X,y,w))\n",
    "        if np.linalg.norm(w_old - w) < tol:\n",
    "            break\n",
    "        w_old = w.copy()\n",
    "    return w, J_hist\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,X):\n",
    "    return np.argmax(X.dot(w), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, N = 5, 500 # number of classes and number of points per class\n",
    "means = [[2, 2], [8, 3], [3, 6], [14, 2], [12, 8]]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "X3 = np.random.multivariate_normal(means[3], cov, N)\n",
    "X4 = np.random.multivariate_normal(means[4], cov, N)\n",
    "X = np.concatenate((X0, X1, X2, X3, X4), axis = 0) # each row is a datapoint\n",
    "Xbar = np.concatenate((X, np.ones((X.shape[0], 1))), axis = 1) # bias trick\n",
    "y = np.asarray([0]*N + [1]*N + [2]*N+ [3]*N + [4]*N) # label\n",
    "W_init = np.random.randn(Xbar.shape[1], C)\n",
    "W, loss_hist = softmax_fit(Xbar, y, W_init, lr = 0.05)\n",
    "\n",
    "y_pred = predict(W,Xbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.676\n"
     ]
    }
   ],
   "source": [
    "count = y==y_pred\n",
    "print(sum(count)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
